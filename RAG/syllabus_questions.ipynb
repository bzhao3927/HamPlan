{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: PyPDF2 in ./.venv/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: openpyxl in ./.venv/lib/python3.12/site-packages (3.1.5)\n",
      "Collecting fpdf\n",
      "  Using cached fpdf-1.7.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from openai) (2.12.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.12/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: fpdf\n",
      "Successfully installed fpdf-1.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai PyPDF2 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/CS/Documents/Deep Learning/Final Project/CPSCI-366_Syllabus.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 3. Extract text from PDF\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     27\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     29\u001b[39m     reader = PyPDF2.PdfReader(f)\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m reader.pages:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Deep Learning/Final Project/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/CS/Documents/Deep Learning/Final Project/CPSCI-366_Syllabus.pdf'"
     ]
    }
   ],
   "source": [
    "# filename: ask_syllabus_questions_save_txt.py\n",
    "\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load OpenAI API key\n",
    "# -----------------------------\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Path to your PDF\n",
    "# -----------------------------\n",
    "pdf_path = \"/Users/CS/Documents/Deep Learning/Final Project/CPSCI-366_Syllabus.pdf\"\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Extract text from PDF\n",
    "# -----------------------------\n",
    "text = \"\"\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    reader = PyPDF2.PdfReader(f)\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define your 10 questions\n",
    "# -----------------------------\n",
    "questions = [\n",
    "    \"When are the lecture sessions for CPSCI-366 held, and where is the classroom located?\",\n",
    "    \"Who is the course instructor, and when can students attend office hours or schedule appointments?\",\n",
    "    \"Describe the structure and purpose of Assignment 2 in the course.\",\n",
    "    \"What is the main goal of the final project in CPSCI-366, and how should students approach it?\",\n",
    "    \"What environment or tools are provided for running deep learning examples and assignments?\",\n",
    "    \"What does the `unet_example.py` file demonstrate, and why is it relevant for the course?\",\n",
    "    \"How should students utilize the `example_data` folder in their assignments or projects?\",\n",
    "    \"Which platforms are used for submitting work and communicating with the instructor?\",\n",
    "    \"How are students‚Äô projects and assignments graded in CPSCI-366?\",\n",
    "    \"Which additional materials or resources are suggested in the syllabus to help students succeed in the course?\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Ask GPT each question and save to text file\n",
    "# -----------------------------\n",
    "output_path = \"syllabus_qa.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, question in enumerate(questions, start=1):\n",
    "        prompt = f\"Answer the following question based only on this syllabus text:\\n\\nSyllabus:\\n{text}\\n\\nQuestion: {question}\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant reading a course syllabus.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        f.write(f\"Q{i}: {question}\\n\")\n",
    "        f.write(f\"A{i}: {answer}\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Q&A saved to text file: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/CS/Documents/Deep Learning/Final Project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY not found in .env file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m client = OpenAI(api_key=OPENAI_API_KEY)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m SYLLABUS_FOLDER = Path(\u001b[34;43m__file__\u001b[39;49m).parent / \u001b[33m\"\u001b[39m\u001b[33msyllabi\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m CACHE_FILE = \u001b[33m\"\u001b[39m\u001b[33msyllabus_embeddings_custom.pkl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m MODEL_PATH = \u001b[33m\"\u001b[39m\u001b[33mfinetuned_syllabus_model\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# filename: rag_with_finetuned_embeddings.py\n",
    "\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "SYLLABUS_FOLDER = Path(__file__).parent / \"syllabi\"\n",
    "CACHE_FILE = \"syllabus_embeddings_custom.pkl\"\n",
    "MODEL_PATH = \"finetuned_syllabus_model\"\n",
    "\n",
    "# Choose which embeddings to use\n",
    "USE_CUSTOM_MODEL = True  # Set to False to use OpenAI embeddings\n",
    "\n",
    "# -----------------------------\n",
    "# PDF Processing\n",
    "# -----------------------------\n",
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a single PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {pdf_path.name}: {e}\")\n",
    "    return text\n",
    "\n",
    "def split_into_chunks(text, chunk_size=500, overlap=100):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_all_syllabi(folder_path):\n",
    "    \"\"\"Load all PDFs from folder and subfolders\"\"\"\n",
    "    documents = []\n",
    "    folder_path.mkdir(exist_ok=True)\n",
    "    pdf_files = list(folder_path.rglob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"\\n‚ö†Ô∏è  No PDF files found in '{folder_path}' or subfolders\")\n",
    "        print(f\"üìÅ Add your syllabus PDFs to: {folder_path.absolute()}\")\n",
    "        return documents\n",
    "    \n",
    "    print(f\"\\nüìÑ Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        rel_path = pdf_file.relative_to(folder_path)\n",
    "        print(f\"üìñ Processing: {rel_path}\")\n",
    "        text = extract_pdf_text(pdf_file)\n",
    "        \n",
    "        if not text.strip():\n",
    "            print(f\"   ‚ö†Ô∏è  No text extracted\")\n",
    "            continue\n",
    "        \n",
    "        chunks = split_into_chunks(text, chunk_size=500, overlap=100)\n",
    "        print(f\"   ‚úì Created {len(chunks)} chunks\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            documents.append({\n",
    "                'text': chunk,\n",
    "                'source': str(rel_path),\n",
    "                'chunk_id': i\n",
    "            })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# -----------------------------\n",
    "# Training Data Generation\n",
    "# -----------------------------\n",
    "def generate_training_data(documents):\n",
    "    \"\"\"Generate synthetic training pairs from syllabi\"\"\"\n",
    "    print(\"\\nüîß Generating training data...\")\n",
    "    \n",
    "    # Create positive pairs: (query, relevant_chunk)\n",
    "    training_examples = []\n",
    "    \n",
    "    # Strategy 1: Use GPT to generate questions for chunks\n",
    "    print(\"   Generating question-answer pairs...\")\n",
    "    \n",
    "    # Sample chunks to avoid cost (take every 5th chunk)\n",
    "    sampled_docs = documents[::5][:50]  # Max 50 chunks\n",
    "    \n",
    "    for i, doc in enumerate(sampled_docs):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"   Progress: {i+1}/{len(sampled_docs)}\")\n",
    "        \n",
    "        # Generate a question for this chunk\n",
    "        prompt = f\"\"\"Generate 1 specific question that could be answered by this text excerpt from a course syllabus. Make it realistic.\n",
    "\n",
    "Text: {doc['text'][:500]}\n",
    "\n",
    "Question:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            question = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Create positive pair\n",
    "            training_examples.append(\n",
    "                InputExample(texts=[question, doc['text']])\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Error generating question: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(training_examples)} training pairs\")\n",
    "    return training_examples\n",
    "\n",
    "# -----------------------------\n",
    "# Fine-tuning\n",
    "# -----------------------------\n",
    "def finetune_model(training_examples):\n",
    "    \"\"\"Fine-tune sentence transformer on syllabus data\"\"\"\n",
    "    print(\"\\nüéì Fine-tuning embedding model...\")\n",
    "    print(\"   This runs locally on your computer (FREE!)\")\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    print(\"   Loading base model: all-MiniLM-L6-v2\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataloader = DataLoader(\n",
    "        training_examples, \n",
    "        shuffle=True, \n",
    "        batch_size=16\n",
    "    )\n",
    "    \n",
    "    # Define loss function\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    \n",
    "    # Train\n",
    "    print(\"   Training started... (this may take 5-15 minutes)\")\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=3,\n",
    "        warmup_steps=100,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(MODEL_PATH)\n",
    "    print(f\"‚úÖ Model saved to: {MODEL_PATH}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Embedding Functions\n",
    "# -----------------------------\n",
    "def get_openai_embedding(text):\n",
    "    \"\"\"Get OpenAI embedding\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(\n",
    "        input=[text], \n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def get_custom_embedding(text, model):\n",
    "    \"\"\"Get custom fine-tuned embedding\"\"\"\n",
    "    return model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "def create_embeddings(documents, use_custom=True):\n",
    "    \"\"\"Create embeddings for all documents - stores BOTH types for comparison\"\"\"\n",
    "    # Always create custom embeddings\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(f\"\\n‚úÖ Loading fine-tuned model from {MODEL_PATH}\")\n",
    "        model = SentenceTransformer(MODEL_PATH)\n",
    "    else:\n",
    "        print(\"\\nüéì No fine-tuned model found. Training new model...\")\n",
    "        training_data = generate_training_data(documents)\n",
    "        model = finetune_model(training_data)\n",
    "    \n",
    "    print(f\"\\nüîÑ Creating custom embeddings...\")\n",
    "    for i, doc in enumerate(documents):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"   Progress: {i+1}/{len(documents)}\")\n",
    "        doc['embedding_custom'] = get_custom_embedding(doc['text'], model)\n",
    "    \n",
    "    # Also create OpenAI embeddings for comparison\n",
    "    print(f\"\\nüîÑ Creating OpenAI embeddings for comparison...\")\n",
    "    for i, doc in enumerate(documents):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"   Progress: {i+1}/{len(documents)}\")\n",
    "        doc['embedding_openai'] = get_openai_embedding(doc['text'])\n",
    "    \n",
    "    # Set default embedding based on mode\n",
    "    for doc in documents:\n",
    "        doc['embedding'] = doc['embedding_custom'] if use_custom else doc['embedding_openai']\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# -----------------------------\n",
    "# RAG Search & Answer\n",
    "# -----------------------------\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def search_documents(query, documents, use_custom=True, top_k=3):\n",
    "    \"\"\"Search for relevant documents\"\"\"\n",
    "    if use_custom:\n",
    "        model = SentenceTransformer(MODEL_PATH)\n",
    "        query_embedding = get_custom_embedding(query, model)\n",
    "        embedding_key = 'embedding_custom'\n",
    "    else:\n",
    "        query_embedding = get_openai_embedding(query)\n",
    "        embedding_key = 'embedding_openai'\n",
    "    \n",
    "    # Calculate scores using appropriate embeddings\n",
    "    results = []\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy['score'] = cosine_similarity(query_embedding, doc[embedding_key])\n",
    "        results.append(doc_copy)\n",
    "    \n",
    "    sorted_docs = sorted(results, key=lambda x: x['score'], reverse=True)\n",
    "    return sorted_docs[:top_k]\n",
    "\n",
    "def answer_question(query, documents, use_custom=True, top_k=3):\n",
    "    \"\"\"Answer question using RAG\"\"\"\n",
    "    relevant_docs = search_documents(query, documents, use_custom, top_k)\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"[Source: {doc['source']}]\\n{doc['text']}\" \n",
    "        for doc in relevant_docs\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Answer the following question based on the provided syllabus excerpts. \n",
    "Cite which course/syllabus your information comes from.\n",
    "If the answer is not in the excerpts, say so.\n",
    "\n",
    "Context from syllabi:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about course syllabi.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    sources = list(set([doc['source'] for doc in relevant_docs]))\n",
    "    \n",
    "    return answer, sources, relevant_docs\n",
    "\n",
    "# -----------------------------\n",
    "# Caching\n",
    "# -----------------------------\n",
    "def save_embeddings(documents, cache_file):\n",
    "    \"\"\"Save embeddings to disk\"\"\"\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    print(f\"\\nüíæ Embeddings cached to {cache_file}\")\n",
    "\n",
    "def load_embeddings(cache_file):\n",
    "    \"\"\"Load cached embeddings\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéì SYLLABUS RAG WITH FINE-TUNED EMBEDDINGS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    mode = \"CUSTOM FINE-TUNED\" if USE_CUSTOM_MODEL else \"OPENAI\"\n",
    "    print(f\"\\nüìä Mode: {mode} embeddings\")\n",
    "    \n",
    "    # Load or create embeddings\n",
    "    documents = load_embeddings(CACHE_FILE)\n",
    "    \n",
    "    if documents is None:\n",
    "        print(\"\\nüîß Building knowledge base...\")\n",
    "        documents = load_all_syllabi(SYLLABUS_FOLDER)\n",
    "        \n",
    "        if not documents:\n",
    "            print(\"\\n‚ùå No syllabi found. Add PDFs to:\", SYLLABUS_FOLDER.absolute())\n",
    "            return\n",
    "        \n",
    "        documents = create_embeddings(documents, use_custom=USE_CUSTOM_MODEL)\n",
    "        save_embeddings(documents, CACHE_FILE)\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Loaded from cache: {len(documents)} chunks\")\n",
    "    \n",
    "    # Interactive chat\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üí¨ Ask questions! Type 'quit' to exit, 'compare' to test both models\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"‚ùì Your question: \").strip()\n",
    "            \n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\nüëã Goodbye!\\n\")\n",
    "                break\n",
    "            \n",
    "            if query.lower() == 'compare':\n",
    "                # Compare both models\n",
    "                query = input(\"‚ùì Question to compare: \").strip()\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"üî¨ COMPARISON MODE\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # Custom model\n",
    "                print(\"\\nüéØ CUSTOM FINE-TUNED MODEL:\")\n",
    "                answer1, sources1, docs1 = answer_question(query, documents, use_custom=True)\n",
    "                print(f\"\\nAnswer: {answer1}\")\n",
    "                print(f\"Sources: {', '.join(sources1)}\")\n",
    "                print(f\"Top score: {docs1[0]['score']:.4f}\")\n",
    "                \n",
    "                # OpenAI model\n",
    "                print(\"\\nüåê OPENAI MODEL:\")\n",
    "                answer2, sources2, docs2 = answer_question(query, documents, use_custom=False)\n",
    "                print(f\"\\nAnswer: {answer2}\")\n",
    "                print(f\"Sources: {', '.join(sources2)}\")\n",
    "                print(f\"Top score: {docs2[0]['score']:.4f}\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "                continue\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nüîç Searching...\")\n",
    "            answer, sources, _ = answer_question(query, documents, use_custom=USE_CUSTOM_MODEL)\n",
    "            \n",
    "            print(f\"\\nüí° Answer:\\n{answer}\")\n",
    "            print(f\"\\nüìö Sources: {', '.join(sources)}\")\n",
    "            print(\"-\" * 60 + \"\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Goodbye!\\n\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-12.0.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.10.23-cp312-cp312-macosx_10_13_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-macosx_10_13_x86_64.whl (182 kB)\n",
      "Downloading regex-2025.10.23-cp312-cp312-macosx_10_13_x86_64.whl (291 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl (454 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-macosx_10_13_x86_64.whl (11 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading pillow-12.0.0-cp312-cp312-macosx_10_13_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-macosx_10_13_x86_64.whl (9.3 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_x86_64.whl (23.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, urllib3, threadpoolctl, sympy, scipy, safetensors, regex, pyyaml, Pillow, networkx, MarkupSafe, joblib, hf-xet, fsspec, filelock, charset_normalizer, scikit-learn, requests, jinja2, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24/24\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 Pillow-12.0.0 charset_normalizer-3.4.4 filelock-3.20.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 pyyaml-6.0.3 regex-2025.10.23 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.3 sentence-transformers-5.1.2 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.2.2 transformers-4.57.1 urllib3-2.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
